{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qakHe57eU50Y"
      },
      "source": [
        "**Importing Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2heGGkUanfYM"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.7.2' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '\"c:/Program Files (x86)/Microsoft Visual Studio/Shared/Python37_64/python.exe\" -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "!pip install klib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnKhfJGo1DpD",
        "outputId": "bffa263c-0173-4428-a89d-290eaef91d32"
      },
      "outputs": [],
      "source": [
        "from nltk.collocations import *\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7rIiipXZKCLx"
      },
      "outputs": [],
      "source": [
        "import concurrent.futures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install pandas \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hURa1oFMKo62"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import klib # install if not loaded '!pip install klib'\n",
        "import spacy\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "R7-DMYinH2VY"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ufbldk56TlHg"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, BatchNormalization, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mp1mXgRUVq6O"
      },
      "outputs": [],
      "source": [
        "# Loading the dataset\n",
        "data = pd.read_csv('consumer_complaints.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sAiBLYAIVcuA",
        "outputId": "0981ee6a-2e64-4bde-87d7-bccd9a843bdf"
      },
      "outputs": [],
      "source": [
        "# First 10 rows of the datset\n",
        "data.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylCXfRdYWJW8",
        "outputId": "a949c791-6369-4c10-bfdc-03d90db3d23d"
      },
      "outputs": [],
      "source": [
        "data.info() # For checking the information about the columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gk-BhO6LWNo0",
        "outputId": "863d6ccc-f174-490a-bc6e-246779be9bec"
      },
      "outputs": [],
      "source": [
        "# FOr checking if dataset contains any null values\n",
        "data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJkMkSjaWci2",
        "outputId": "2e2775f8-7bcb-47b1-e2ca-78eb6efb4614"
      },
      "outputs": [],
      "source": [
        "# To drop the duplicate rows, dropping missing data, reducing memory usage.\n",
        "data = klib.data_cleaning(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVqHcIbYC7EM",
        "outputId": "08d44e9f-02ce-49bd-b67f-b9f8c6e1aa40"
      },
      "outputs": [],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "yGaKEd2NXpMl"
      },
      "outputs": [],
      "source": [
        "# to clean the column names like converting it into lowercase\n",
        "data = klib.clean_column_names(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Dyuk9Il2YwQO"
      },
      "outputs": [],
      "source": [
        "# Converting the datatypes into appropriate datatypes\n",
        "data = klib.convert_datatypes(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "0WV7p1XBZB9S"
      },
      "outputs": [],
      "source": [
        "# Dropping Rows which have missing values in all columns\n",
        "data = klib.drop_missing(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DiNZRlBZH9o",
        "outputId": "ee6ff2f7-6ddf-459b-e4ed-35903050d0c8"
      },
      "outputs": [],
      "source": [
        "# Dataframe shape\n",
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXYyBVbjaSJl",
        "outputId": "31532808-9045-45f0-dc56-32c4f536d01a"
      },
      "outputs": [],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNYXpNqgF1of"
      },
      "outputs": [],
      "source": [
        "columns_to_convert = [\n",
        "    'date_received', 'product', 'sub_product', 'issue', 'sub_issue',\n",
        "    'company_public_response', 'company', 'state', 'zip_code', 'tags',\n",
        "    'consumer_consent_provided', 'submitted_via', 'date_sent_to_company',\n",
        "    'company_response_to_consumer', 'timely_response'\n",
        "]\n",
        "\n",
        "# Convert specified columns to object\n",
        "data[columns_to_convert] = data[columns_to_convert].astype(object)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "PdRG6BgyZK5e"
      },
      "outputs": [],
      "source": [
        "# I want to drop all those rows which have null values in my target column\n",
        "data.dropna(subset = ['consumer_disputed'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3c9bw5_aE-v",
        "outputId": "cf92c4ee-3520-446b-bb8d-1abd380c1b7b"
      },
      "outputs": [],
      "source": [
        "data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPvEps31aiWd",
        "outputId": "dbd1fec1-4e0a-4c05-df66-66e37cde62e9"
      },
      "outputs": [],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "0f-RaENs3E4h",
        "outputId": "c60a8ccf-2a89-4921-e57f-91fd47d4e6df"
      },
      "outputs": [],
      "source": [
        "data.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUTDIa5Xpb7T",
        "outputId": "f0dd2adc-c71b-4ca7-c653-f4c03b05bdc0"
      },
      "outputs": [],
      "source": [
        "data.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Td2qVQcJ5wea"
      },
      "source": [
        "### **Handelling Missing Values**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Tg_z8HRVAFs"
      },
      "source": [
        "As I have huge dataset, and modelling with all the columns is not feasiable, so we can drop few columns which are unnecessary for us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKnKeDhG5b4B",
        "outputId": "43ca7c93-33ac-4e94-caf6-9dfa9f419d4d"
      },
      "outputs": [],
      "source": [
        "data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "nONd9MaQ4bnj"
      },
      "outputs": [],
      "source": [
        "# Correcting column names to match those in the DataFrame\n",
        "clean_df = data.drop(columns=['consumer_consent_provided', 'complaint_id', 'date_sent_to_company',\n",
        "                              'zipcode', 'state', 'date_received', 'sub_product', 'consumer_complaint_narrative',\n",
        "                              'sub_issue'])\n",
        "# Removing rows with any missing values\n",
        "clean_df.dropna(inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqUJqWdYT6k0",
        "outputId": "acd346bb-2bd5-4a58-fbf1-b0f50a206132"
      },
      "outputs": [],
      "source": [
        "clean_df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NggLB4kR5RTZ",
        "outputId": "c27b123f-43c0-44b3-b3e9-23c0738606c2"
      },
      "outputs": [],
      "source": [
        "clean_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "bAW6zvE5HgiB"
      },
      "outputs": [],
      "source": [
        "clean_df = clean_df.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "id": "DmMn164lUpVv",
        "outputId": "7c04e374-22f1-49ac-f9b2-12acfa969d80"
      },
      "outputs": [],
      "source": [
        "clean_df.head(100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUkJMTgoV3-Z"
      },
      "source": [
        "ideas ~\n",
        "\n",
        "1. Either we can delete 'consumer_complaint_narrative' or we have to do a sentiment analysis on this column and classify first this texts into positive, negative and neutral then do the process\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yij9horSGOMY"
      },
      "source": [
        "Is the target column balanced or not ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDXC-44zGUgT",
        "outputId": "3e4b8dee-0e7a-498d-a2f2-d75a2344354a"
      },
      "outputs": [],
      "source": [
        "clean_df['consumer_disputed'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifKrnoKrGX5V"
      },
      "source": [
        "Clearly dataframe is imbalanced. So we can consider the following methods for correcting this issue\n",
        "\n",
        "- Synthetic Minority Over-sampling Technique (SMOTE)\n",
        "- Random Over Sampler\n",
        "- Random Forests or Support Vector Machines\n",
        "- Stratified Sampling\n",
        "- Ensemble Methods like Bagging and Boosting\n",
        "- providing class weights to penalize misclassifications of the minority class more heavily"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pXV2ClaXGV-"
      },
      "source": [
        "## Data Cleaning & Pre Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iffCUW_ZsYWG"
      },
      "source": [
        "---\n",
        "Checking the product columns for valid product names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIBcv7LAsuMn"
      },
      "outputs": [],
      "source": [
        "print('Product')\n",
        "for i in clean_df['product'].unique():\n",
        "    print(i)\n",
        "\n",
        "print('```````````````````````````````````````````````````````')\n",
        "\n",
        "print('timely_response')\n",
        "for i in clean_df['timely_response'].unique():\n",
        "    print(i)\n",
        "\n",
        "print('```````````````````````````````````````````````````````')\n",
        "\n",
        "print('tags')\n",
        "for i in clean_df['tags'].unique():\n",
        "    print(i)\n",
        "\n",
        "print('```````````````````````````````````````````````````````')\n",
        "\n",
        "print('company_response_to_consumer')\n",
        "for i in clean_df['company_response_to_consumer'].unique():\n",
        "    print(i)\n",
        "\n",
        "print('```````````````````````````````````````````````````````')\n",
        "\n",
        "print('submitted_via')\n",
        "for i in clean_df['submitted_via'].unique():\n",
        "    print(i)\n",
        "\n",
        "print('```````````````````````````````````````````````````````')\n",
        "\n",
        "print('consumer_disputed')\n",
        "for i in clean_df['consumer_disputed'].unique():\n",
        "    print(i)\n",
        "\n",
        "print('```````````````````````````````````````````````````````')\n",
        "\n",
        "print('company_public_response')\n",
        "for i in clean_df['company_public_response'].unique():\n",
        "    print(i)\n",
        "\n",
        "print('```````````````````````````````````````````````````````')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqLJL5tnuAtL"
      },
      "outputs": [],
      "source": [
        "clean_df['tags'] = clean_df['tags'].replace({'Older American, Servicemember': 'Older American and Servicemember'})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln9_S0Ep_yCP"
      },
      "source": [
        "Performing all the NLP preprocessing tasks\n",
        "- Removing punctuations\n",
        "- Tokenization\n",
        "- Lower Casing\n",
        "- Stop Word Removal\n",
        "- Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "TBsAzNTxIl-T"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame.copy(clean_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "gm_BKS230Klz"
      },
      "outputs": [],
      "source": [
        "stemmer = PorterStemmer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = ''.join([char for char in text if char not in string.punctuation])\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word.lower() for word in tokens]\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
        "    tokens = [stemmer.stem(word) for word in tokens]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "def parallel_preprocess(column):\n",
        "    df[column] = df[column].apply(preprocess_text)\n",
        "\n",
        "columns_to_preprocess = ['product', 'issue', 'company_public_response', 'company', 'tags', 'submitted_via', 'company_response_to_consumer', 'timely_response']\n",
        "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "    executor.map(parallel_preprocess, columns_to_preprocess)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDdjug-aFoX5"
      },
      "source": [
        "**Vectorizing the dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKriwad5Vket"
      },
      "source": [
        "Changing the target column into 0 & 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "2pM9WUhGSktG"
      },
      "outputs": [],
      "source": [
        "df['consumer_disputed'] = df['consumer_disputed'].apply(lambda x: 1 if x == 'Yes' else 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOyFBC2EVjLe"
      },
      "source": [
        "**Now we can use WordToVec for embedding the other columns**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "e4CXDz7rVH8d",
        "outputId": "001b91a0-30dd-4511-a71c-4a969c0bfa11"
      },
      "outputs": [],
      "source": [
        "df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt', download_dir='C:/nltk_data')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt', download_dir='C:/nltk_data')\n",
        "nltk.data.path.append('C:/nltk_data')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nltk.download('punkt', download_dir='C:/nltk_data')\n",
        "nltk.data.path.append('C:/nltk_data')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "print(nltk.data.path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.data.path.append('C:/nltk_data')  # Adjust the path if necessary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "XHeLs-3iOpH6"
      },
      "outputs": [],
      "source": [
        "df['combined_text'] = df['product'].astype(str) + ' ' + df['issue'].astype(str) + ' ' + df['company_public_response'].astype(str) + ' ' + df['company'].astype(str) + ' ' + df['tags'].astype(str) + ' ' + df['submitted_via'].astype(str) + ' ' + df['company_response_to_consumer'].astype(str) + ' ' + df['timely_response'].astype(str)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "df['tokenized_text'] = df['combined_text'].apply(word_tokenize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSv3DwwdR6gN"
      },
      "outputs": [],
      "source": [
        "w2v_model = Word2Vec(sentences = df['tokenized_text'], vector_size=100, window=5, min_count=1, sg=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pt-UqQ72SGBH"
      },
      "outputs": [],
      "source": [
        "word_embeddings = [w2v_model.wv[word] for word in df['tokenized_text']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPFKwxw71GcO"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "w2v_model.save(\"word2vec_model.bin\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijU7NI54SLdC"
      },
      "outputs": [],
      "source": [
        "document_embeddings = [np.mean(embeddings, axis=0) if embeddings.any() else np.zeros(w2v_model.vector_size) for embeddings in word_embeddings]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JS0znS73SSFY"
      },
      "outputs": [],
      "source": [
        "df['document_embeddings'] = document_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "ZHnXxBvOSUi1",
        "outputId": "d51af61d-f50e-436a-a950-cfa3a33a886d"
      },
      "outputs": [],
      "source": [
        "df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWFw4QCfzlSP",
        "outputId": "aecdab8d-40c2-4ca0-85a0-ded7c99eb042"
      },
      "outputs": [],
      "source": [
        "len(df['document_embeddings'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSd0OJ3HS4Kc"
      },
      "source": [
        "# **Modelling**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlOMWQ2lZu9i",
        "outputId": "be7cd04f-3f9d-4052-b6af-61819758554d"
      },
      "outputs": [],
      "source": [
        "l = [len(i) for i in df['document_embeddings']]\n",
        "print(sorted(l)[int(len(l)/2)])\n",
        "print(len(df['document_embeddings'][0]))\n",
        "print(len(df['document_embeddings']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3b0_DVOcmn9"
      },
      "source": [
        "Total length 29480, each row length 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LImSIeQSWvM"
      },
      "outputs": [],
      "source": [
        "X = df['document_embeddings']\n",
        "y = df['consumer_disputed']\n",
        "X = np.array(X)\n",
        "y = np.array(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcWT6V0rTVjx"
      },
      "source": [
        "**RandomOverSampler is a technique used in machine learning to handle imbalanced datasets. It works by randomly duplicating instances of the minority class in the dataset until it is balanced with the majority class.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49n0x9NSUWd1",
        "outputId": "fae2d593-598f-42c3-8107-1f163582f7dc"
      },
      "outputs": [],
      "source": [
        "X.shape,y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhJeTGLDTJJV"
      },
      "outputs": [],
      "source": [
        "ros = RandomOverSampler(random_state=42)\n",
        "X_resampled, y_resampled = ros.fit_resample(X.reshape(-1, 1), y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cc9keVYgHDU",
        "outputId": "f8e5be37-7074-4ad9-c6b4-5a21ec1af6ad"
      },
      "outputs": [],
      "source": [
        "X_train = np.array(X_train.tolist())\n",
        "X_test = np.array(X_test.tolist())\n",
        "X_train.shape, X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpdpZtu4TJ-e",
        "outputId": "aac0e228-0453-4e44-9f35-ca4782a69bec"
      },
      "outputs": [],
      "source": [
        "train_class_distribution = np.bincount(y_train)\n",
        "test_class_distribution = np.bincount(y_test)\n",
        "\n",
        "print(f\"Train Class Distribution: {train_class_distribution}\")\n",
        "print(f\"Test Class Distribution: {test_class_distribution}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6KDEHVwfqxW"
      },
      "outputs": [],
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 100, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 100, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3zKzLiHhyFD",
        "outputId": "b9c73272-a4da-45d0-d416-afa9825a6670"
      },
      "outputs": [],
      "source": [
        "early_stopping = EarlyStopping(monitor = 'accuracy' , patience = 1  ,restore_best_weights = True )\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(64, return_sequences=True))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(32))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "Model_hist= model.fit(X_train, y_train, epochs=10, batch_size=32,\n",
        "                      validation_data=(X_test, y_test),verbose=1,callbacks=[early_stopping])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nc-VpXaQiDgC",
        "outputId": "ffd61ccf-3dcb-40dc-f538-ffb00504130c"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(X_test)\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "y_pred = (y_pred > 0.2).astype('int32')\n",
        "classification_rep = classification_report(y_test, y_pred)\n",
        "\n",
        "print(classification_rep)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzx7rWJFyk8t",
        "outputId": "ab284170-1329-4ef1-eab3-9125c97231c0"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(X_test)\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "y_pred = (y_pred > 0.75).astype('int32')\n",
        "classification_rep = classification_report(y_test, y_pred)\n",
        "\n",
        "print(classification_rep)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "6JEvJbxajE3_",
        "outputId": "8a29f913-64e1-4d9e-bc57-19ec54f0cb5f"
      },
      "outputs": [],
      "source": [
        "plt.plot(Model_hist.history['loss'])\n",
        "plt.plot(Model_hist.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "-B8HXMFXkliB",
        "outputId": "dc83198c-4bcc-4a65-942f-2f5d3a069b6d"
      },
      "outputs": [],
      "source": [
        "plt.plot(Model_hist.history['accuracy'])\n",
        "plt.plot(Model_hist.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIQU5-tckp1y",
        "outputId": "270a84db-8659-40e0-dae2-1318bf35fdd6"
      },
      "outputs": [],
      "source": [
        "model.save('lstm_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVyQZHhuzFlR"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "loaded_model = load_model('lstm_model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irHkrNBZ0bSc"
      },
      "source": [
        "Further Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qv3MeU-1Pa5"
      },
      "outputs": [],
      "source": [
        "w2v_model = Word2Vec.load(\"word2vec_model.bin\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxQNttd2zPzQ"
      },
      "outputs": [],
      "source": [
        "example_text = \"mortgag Loan servicing, payments, escrow account compani choos provid public respons bank america nation associ older american postal mail close explan ye\"\n",
        "\n",
        "\n",
        "tokenized_text = word_tokenize(example_text.lower())  # Convert to lowercase for consistency\n",
        "\n",
        "word_embeddings_2 = [w2v_model.wv[word] for word in tokenized_text if word in w2v_model.wv]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKz5RgMZ2R6s"
      },
      "outputs": [],
      "source": [
        "document_embeddings_2 = [np.mean(embeddings, axis=0) if embeddings.any() else np.zeros(w2v_model.vector_size) for embeddings in word_embeddings_2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjr6FTft9Ba7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assuming 'example_text' is the text you want to convert\n",
        "example_tokens = word_tokenize(example_text)\n",
        "\n",
        "# Initialize an empty list to store word vectors\n",
        "example_vectors = []\n",
        "\n",
        "# Iterate through each token in the example text\n",
        "for token in example_tokens:\n",
        "    try:\n",
        "        vector = w2v_model.wv[token]\n",
        "        example_vectors.append(vector)\n",
        "    except KeyError:\n",
        "        # Handle the case where a token is not in the vocabulary\n",
        "        pass\n",
        "\n",
        "# If there are no vectors for any tokens, add a zero vector\n",
        "if not example_vectors:\n",
        "    example_vectors.append(np.zeros(w2v_model.vector_size))\n",
        "\n",
        "# Calculate the mean of the word vectors\n",
        "example_vector = np.mean(example_vectors, axis=0)\n",
        "\n",
        "# 'example_vector' now contains the vector representation of the example text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MU0w647n_St3"
      },
      "outputs": [],
      "source": [
        "vec = np.array(example_vector)\n",
        "\n",
        "final_vec= np.expand_dims(vec, axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hl2o46Wc_mrP",
        "outputId": "8c0942d0-6135-4411-b0f0-5799108bdfdf"
      },
      "outputs": [],
      "source": [
        "final_vec.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q70hTYVCA7X0"
      },
      "outputs": [],
      "source": [
        "final_vec = vec.reshape((1, vec.shape[0],1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tT-KF-uVBEVP",
        "outputId": "b2eae0ca-b9f2-4f3e-e658-06b78db0ee77"
      },
      "outputs": [],
      "source": [
        "pred = model.predict(final_vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1r5GWLrBIjX",
        "outputId": "cc9391ec-8fd6-4804-a5a8-421a4b7c583f"
      },
      "outputs": [],
      "source": [
        "pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "I777w3DABKnQ",
        "outputId": "ed057c1e-7bc4-4860-dfff-a59414647349"
      },
      "outputs": [],
      "source": [
        "pred_class = 'Yes'\n",
        "if pred[0][0]<0.5:\n",
        "    pred_class = 'Yes'\n",
        "else:\n",
        "    pred_class = 'No'\n",
        "\n",
        "pred_class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYt3eaeEIlHJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
